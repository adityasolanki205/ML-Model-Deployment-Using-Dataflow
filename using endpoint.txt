import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions
from google.cloud import aiplatform
from google.cloud import bigquery
import json

# Define the pipeline options
class CustomPipelineOptions(PipelineOptions):
    @classmethod
    def _add_argparse_args(cls, parser):
        parser.add_argument('--project_id')
        parser.add_argument('--region')
        parser.add_argument('--bucket_name')
        parser.add_argument('--vertex_model_id')
        parser.add_argument('--bq_dataset_id')
        parser.add_argument('--bq_table_id')

# Preprocessing function
def preprocess_data(record):
    # Here you can add any preprocessing logic (e.g., cleaning, transformations)
    # This is just a placeholder
    return record.strip()

# Function to call Vertex AI for inference
def get_inference(record, model_id, project_id, region):
    # Initialize the Vertex AI client
    aiplatform.init(project=project_id, location=region)
    
    # Assuming the model expects a json payload
    endpoint = aiplatform.Endpoint(model=model_id)

    # Call the endpoint for prediction
    prediction = endpoint.predict(instances=[record])
    
    # Process the prediction result (e.g., get the first prediction result)
    return prediction.predictions[0]

# Function to write the output to BigQuery
def write_to_bigquery(record, project_id, dataset_id, table_id):
    client = bigquery.Client(project=project_id)
    table_ref = client.dataset(dataset_id).table(table_id)
    
    # Assuming the record is a dictionary that can be written to BigQuery
    errors = client.insert_rows_json(table_ref, [record])
    
    if errors:
        print(f"Error inserting rows into BigQuery: {errors}")
    else:
        print(f"Successfully inserted record into BigQuery")

# The main pipeline function
def run(argv=None):
    # Setup pipeline options
    pipeline_options = PipelineOptions(argv)
    custom_options = pipeline_options.view_as(CustomPipelineOptions)
    
    # Input GCS path (your files on GCS)
    gcs_input_path = f"gs://{custom_options.bucket_name}/input_data/*"
    
    # Initialize the pipeline
    with beam.Pipeline(options=pipeline_options) as p:
        # Step 1: Read data from GCS
        records = (
            p | 'Read from GCS' >> beam.io.ReadFromText(gcs_input_path)
        )
        
        # Step 2: Preprocess data
        preprocessed_data = (
            records | 'Preprocess Data' >> beam.Map(preprocess_data)
        )
        
        # Step 3: Get Inference from Vertex AI
        inference_results = (
            preprocessed_data 
            | 'Get Inference' >> beam.Map(get_inference, 
                                          model_id=custom_options.vertex_model_id,
                                          project_id=custom_options.project_id, 
                                          region=custom_options.region)
        )
        
        # Step 4: Write results to BigQuery
        inference_results | 'Write to BigQuery' >> beam.Map(write_to_bigquery,
                                                              project_id=custom_options.project_id,
                                                              dataset_id=custom_options.bq_dataset_id,
                                                              table_id=custom_options.bq_table_id)

if __name__ == '__main__':
    run()
